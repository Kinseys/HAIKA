# Autoencoder Pipeline for Morphology Analysis
<p align="justify">
This repository provides a complete workflow for <strong>image preprocessing</strong>, <strong>deep feature extraction</strong>, and <strong>unsupervised clustering</strong> for insect morphology studies by using the wings images. The pipeline standardizes raw images, learns compact latent â€œfingerprintsâ€ using a convolutional autoencoder, and then groups similar specimens through clustering methods such as OPTICS, KMeans, and Agglomerative clustering.
</p>
This framework was developed in collaboration with MusÃ©um national d'Histoire naturelle (MNHN) teams, University of Oxford and INSERM. It's designed for <strong>reproducibility</strong>, <strong>scalability</strong>, and <strong>biological interpretability</strong>.


<p align="center">
  <img src="img/model.png" width="90%">
</p>

---

## âœ¨ Features

- **Unified two-step pipeline**
  - **Preprocessing**: format unification, resizing, renaming.
  - **Autoencoder representation learning**: compact latent embeddings.
  - **Clustering**: OPTICS, KMeans, Agglomerative, with metrics and visualizations.

- **Rich visual outputs**
  - Activation maps  
  - Attention / importance heatmaps  
  - t-SNE / PCA projections  
  - Clustering diagnostics

- **Reproducible model + feature outputs**
  - Embedding files  
  - Model checkpoints  
  - Excel summary reports  

---

## ğŸ“¦ Requirements

Python **3.9+**

Install all dependencies:

```bash
pip install -r requirements.txt
```

---

## ğŸ“ Repository Structure

```
â”œâ”€â”€ preprocess.py           # Preprocessing raw images
â”œâ”€â”€ main.py                 # Autoencoder + clustering pipeline
â”œâ”€â”€ user_guide.docx         # Full detailed documentation
â”œâ”€â”€ README.md               # Project introduction
â””â”€â”€ (Output folders created after running the scripts)
```

---

# Part 1 â€” Preprocessing (`preprocessing.py`)

<p align="center">
  <img src="img/preprocessing.png" width="90%">
</p>


### ğŸ”§ What it does
- Scans all subfolders for images  
- Resizes & converts images to PNG  
- Standardizes naming scheme  
- Produces dataset statistics & label summaries  

### ğŸ”‘ Key Settings

| Setting | Meaning | Suggested |
|--------|---------|-----------|
| `input_dir` | Raw image folder | Your dataset path |
| `output_dir` | Output folder | e.g., `D:\preprocessing` |
| `IMAGE_SIZE` | Final image size | Match training resolution |
| `SUPPORTED_FORMATS` | Allowed types | png, jpg, jpeg, bmp, tif |

### â–¶ï¸ Run

```bash
python preprocess.py
```

### ğŸ“¤ Outputs
- Resized PNGs  
- `statistics.csv`  
- `family_counts.csv`, `species_counts.csv`  
- `unique_families.txt`, `unique_species.txt`, `unique_names.txt`  
- `errors.log`  

---

# Part 2 â€” Autoencoder + Clustering (`main.py`)

### ğŸ¯ Goal
Train an autoencoder to extract morphological features, then cluster them to reveal structure in the dataset.


### ğŸ§  Model Structure
A convolutional autoencoder compresses each image into a low-dimensional latent vector.  
These embeddings are clustered and visualized using multiple methods.

<p align="center">
  <img src="img/model_01.png" width="90%">
</p>

### ğŸ”‘ Hyperparameters

| Param | Location | Description | Recommendation |
|-------|----------|-------------|----------------|
| `data_folder` | in `main()` | Input processed images | Use preprocess output |
| `OUTPUT_DIR` | top of file | Save path | New folder per run |
| `IMG_SIZE` | top of file | Training size | 128â€“224 |
| `LATENT_DIM` | top of file | Embedding length | 32â€“128 |
| `BATCH_SIZE` | top of file | Step size | 8â€“32 |
| `EPOCHS` | top of file | Training epochs | 40â€“100 |
| `REG_FACTOR` | top of file | Weight decay | 1e-5 â†’ 5e-5 |

### â–¶ï¸ Run

```bash
python main.py
```

---

# ğŸ“‚ Output Directory Overview

All results are saved under your chosen `OUTPUT_DIR`.

### ğŸ” Main Files & Their Use

| File/Folder | Description | Use |
|-------------|-------------|-----|
| `autoencoder.h5` | Full model | Reconstruction / further training |
| `encoder.h5` | Encoder-only | Extract embeddings for new images |
| `image_embeddings.txt` | Latent vectors | Clustering & ML |
| `activations/` | Filter activations | Interpret model focus |
| `heatmaps/` | Attention overlays | Morphological relevance |
| `tsne_plots/` | 2D visualizations | Inspect global structure |
| `clustering_results.xlsx` | Cluster labels & coordinates | Main summary |
| `clustering_metrics.xlsx` | Silhouette, CH, DBI | Compare cluster quality |
| `clustering_metrics.png` | Bar chart | Quick inspection |

---

# ğŸ“˜ How to Interpret Results

### `clustering_results.xlsx`
Contains:
- filename  
- family / species / name  
- cluster labels (OPTICS, KMeans, Agglomerative)  
- t-SNE / PCA 2D coordinates  

### `clustering_metrics.xlsx`
- `silhouette` â†’ higher = better  
- `calinski_harabasz` â†’ higher = better  
- `davies_bouldin` â†’ lower = better  

---

# ğŸ”§ Hyperparameter Tips

1. Start small: `128Ã—128`, `LATENT_DIM=64`, `BATCH_SIZE=16`, `EPOCHS=40`.  
2. Blurry reconstructions â†’ increase `LATENT_DIM` or `EPOCHS`.  
3. Noisy heatmaps â†’ increase `REG_FACTOR`.  
4. Compare clustering metrics to choose the best run.  
5. Use a fresh `OUTPUT_DIR` per experiment.

---

# ğŸ‘€ Visual Outputs

- **Activation maps** â†’ internal filter responses
<p align="center">
  <img src="img/Condylognatha-Achilidae_01_activations.png" width="80%">
</p>
  
- **Heatmaps** â†’ important morphological regions

<p align="center">
  <img src="img/heatmap.png" width="80%">
</p>

- **t-SNE & PCA** â†’ visual grouping of specimens  


---

# â— Troubleshooting

- No images found â†’ check `data_folder`  
- Out of memory â†’ reduce `IMG_SIZE` or `BATCH_SIZE`  
- OPTICS returns many `-1` â†’ try KMeans/Agglomerative  
- Windows path issues â†’ use double `\\` (e.g., `D:\\data\\images`)  



#

---
<br><br><br><br>
---

# Wing Segmentation and Morphological Analysis

The segmentation folder contains two core scripts that perform **automatic insect wing segmentation** and **biomechanical property estimation**.

---

## `seg_main.py`

This script performs **wing segmentation using the Segment Anything Model (SAM)**.  
It automatically generates masks for each input image and extracts feature embeddings for further analysis.

**Main steps:**
1. Load and preprocess all insect wing images.  
2. Use the Meta AI **Segment Anything (ViT-B)** model to detect and segment individual wing regions.  
3. Save all mask files for each specimen in a dedicated subfolder (`*_masks/`).  
4. Compute embeddings for each segmented region using a pretrained **MobileNetV2** feature extractor.  
5. Visualize species clustering using **PCA** and **t-SNE** projections.

**Output examples:**
- Segmentation masks (per image)  
- Extracted feature CSV files  
- Clustering visualizations (`PCA`, `t-SNE`, `dendrograms`)  

ğŸ“ **Example output structure:**
```
SEG_all_new/
 â”œâ”€â”€ image1_masks/
 â”‚   â”œâ”€â”€ image1_mask_0.png
 â”‚   â”œâ”€â”€ image1_mask_1.png
 â”‚   â””â”€â”€ ...
 â”œâ”€â”€ extracted_features.csv
 â””â”€â”€ cluster_labels_with_pca_tsne.csv
```
<p align="center">
  <img src="img/segments.png" width="90%">
  <br>
  <em>Figure. Automatic segmentation.</em>
</p>
---

### `outline_extract_and_calculate.py`

This script post-processes the SAM-generated masks to **extract wing outlines** and **compute morphological flight metrics**.

**Main functions:**
1. Combine individual mask regions for each specimen.  
2. Extract the **largest contour** representing the full wing area.  
3. Compute key aerodynamic parameters:  
   - **Wing span (b)**  
   - **Wing area (S)**  
   - **Average chord (c = S/b)**  
   - **Aspect ratio (AR = bÂ²/S)**  
4. Estimate **body volumes and flight efficiency** based on wing span scaling laws.  
5. Save all wing outline images and parameter reports (`TXT`, `Excel`).

**Output examples:**
- Outlined wing images (black on white background)  
- `wing_body_parameters.xlsx` â€” including span, area, aspect ratio, and flight efficiency  
- `wing_body_parameters.txt` â€” detailed per-image summaries  

ğŸ“ **Example output structure:**
```
Wing_Outlines_new/
 â”œâ”€â”€ image1_combined_outline.png
 â”œâ”€â”€ wing_body_parameters.xlsx
 â””â”€â”€ wing_body_parameters.txt
```

---

### ğŸ§­ Workflow Summary

1. Run `seg_main.py` â†’ performs segmentation and feature extraction.  
2. Run `outline_extract_and_calculate.py` â†’ extracts outlines and computes morphological/flight traits.  

ğŸ“· **Example illustration:**
<p align="center">
  <img src="img/outlines.png" width="90%">
  <br>
  <em>Figure. Automatic segmentation and morphological parameter extraction.</em>
</p>



---
<br><br><br><br>

# ğŸ§Š 3D Extension (Volumetric Shapes and primates dataset)
<p align="justify">
The repository includes a minimal, self-contained <strong>3D generalization</strong> demo under <strong>`3d_expansion`</strong>.  
It shows that our representation-learning + clustering pipeline <strong>extends naturally to 3D morphology and primates 3d data </strong> by learning latent codes from <strong>synthetic volumetric shapes</strong> (e.g., sphere, cube, ellipsoid, cylinder, torus; and cuneiform, femur, mandible, patella, scapula, vertebra for primates).
</p>
<table align="center">
  <tr>
    <td align="center">
      <img src="3d_expansion/primates/cuneiform.png" width="200px"><br>
      <b>Cuneiform</b>
    </td>
    <td align="center">
      <img src="3d_expansion/primates/femur.png" width="200px"><br>
      <b>Femur</b>
    </td>
    <td align="center">
      <img src="3d_expansion/primates/mandible.png" width="200px"><br>
      <b>Mandible</b>
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="3d_expansion/primates/vertebra.png" width="200px"><br>
      <b>Vertebra</b>
    </td>
    <td align="center">
      <img src="3d_expansion/primates/patella.png" width="200px"><br>
      <b>Patella</b>
    </td>
    <td align="center">
      <img src="3d_expansion/primates/scapula.png" width="200px"><br>
      <b>Scapula</b>
    </td>
  </tr>
</table>
<p align="center">
  <em>3D primates datasets examples. The raw volume data were downloaded from MorphoSource (ID: 000656244; AlmÃ©cija et al. 2024).</em>
</p>
---

### ğŸ“‚ Folder
```
3d_extensioni/
 â”œâ”€â”€ main_3d.py          # 3D synthetic shapes + 3D autoencoder + projections (t-SNE/PCA)
 â”œâ”€â”€ primates/           # basic results for primates
 â””â”€â”€ (generated outputs after running)
```

---

### â–¶ï¸ How to run
From the project root:
```bash
python main_3d.py
```

This will:
1) **Generate synthetic 3D shapes** (voxel grids)  
2) **Train a lightweight 3D autoencoder** to learn latent features  
3) **Project latent codes** with t-SNE/PCA for visualization  
4) **Save plots** under `outputs_3d_demo/`

Expected outputs:
```
outputs_3d_demo/
 â”œâ”€â”€ tsne_3d.png   # t-SNE on latent features (colored by shape)
 â””â”€â”€ pca_3d.png    # PCA on latent features (colored by shape)
```

---

### ğŸ“ What this demonstrates
- The same **autoencoder-based feature learning** strategy works on **3D volumes**.  
- Shape classes form **separable clusters** in the learned latent space (see t-SNE/PCA).  
- This provides a simple proof-of-concept that our 2D pipeline **generalizes to 3D morphology**.

---

### ğŸ“· visualization results for volume data
<p align="center">
  <img src="img/tsne_3d.png" width="45%">
  <img src="img/pca_3d.png"  width="45%">
  <br>
  <em>3D latent projections: t-SNE (left) and PCA (right).</em>
</p>

### ğŸ“· visualization results for primates data
<p align="center">
  <img src="3d_expansion/primates/tsne.png" width="45%">
  <img src="3d_expansion/primates/pca.png"  width="45%">
  <br>
  <em>3D latent projections: t-SNE (left) and PCA (right).</em>
</p>






---

# ğŸ“„ Citation

If you use this pipeline in research, please cite:

---
# ğŸ“„ References
AlmÃ©cija, S., Pugh, K. D., Anaya, A., Smith, C. M., Simmons, N. B., Voss, R. S., ... & Catalano, S. A. (2024). Primate Phenotypes: A Multi-Institution Collection of 3D Morphological Data Housed in MorphoSource. Scientific Data, 11(1), 1391.

---

# ğŸ“œ License

MIT License  

---

# ğŸ™‹ Contact

For questions, suggestions, or collaborations, please feel free to open an issue or contact the author shihan.guan@univ-rennes.fr.
